---
title: "Example: Video Dubbing"
description: "Build a video dubbing Sieve app"
---

In this example, we will explore a more complex use case of Sieve. We’ll create an application that automatically dubs a video of a person speaking into another language! We’ll take an input video, transcribe it, translate that text to another language, generate text-to-speech of that translated text, and have the original video lipsync to the new audio.

By following this, you will learn how to:

- Use the Sieve client package to call multiple existing functions in pure Python
- Combine these functions to create a customized app that meets your requirements

## **Introduction**

As mentioned above, to create an app that can dub videos, we need several models to work together:

1. ****************WhisperX****************: an audio transcription model
2. ************************SeamlessT2T:************************ a text-to-text translation model
3. ******************XTTS-V1:****************** a text-to-speech model
4. ******************************************Sieve Video Retalker:****************************************** an optimized version of video retalker for lipsyncing

## **Using the python client**

Ensure that the Sieve python package is installed on your system by executing the following command.

```python
pip show sievedata
```

If installed successfully, you will see an output detailing the package's version, summary, dependencies, etc. If it isn’t, please follow installation instructions linked here.

## Building **the app from scratch**

Create a new Python file named **`video_dubbing.py`** with the following command:

```bash
touch video_dubbing.py
```

Open**`video_dubbing.py`** and paste the following code:

```python
import sieve


def extract_audio(source_video: sieve.Video):
    import subprocess
    audio_path = 'temp.wav'
    subprocess.run(["ffmpeg", "-i", source_video.path, audio_path, "-y"])
    return sieve.Audio(path=audio_path)


def transcript_to_text(transcript):
    transcript = [segment for sublist in transcript for segment in sublist]
    text = " ".join([segment["text"] for segment in transcript])
    return text


seamless_language_map = {
    "en": "eng",
    "es": "spa",
    "fr": "fra",
    "de": "deu",
    "it": "ita",
    "pt": "por",
    "pl": "pol",
    "tr": "tur",
    "ru": "rus",
    "nl": "nld",
    "cs": "ces",
    "ar": "ara",
    "zh-cn": "cmn"
}


def video_dubbing(source_video: sieve.Video, language: str):
    """
    :param source_video: The video to dub
    :param language: The language to dub the video in
    :return: The dubbed video
    """

    # Extract audio from video
    source_audio = extract_audio(source_video)

    # check if language is supported
    if language not in ["en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru", "nl", "cs", "ar", "zh-cn"]:
        raise Exception("Language not supported")

    # use remote sieve models
    transcriber = sieve.function.get("sieve/speech_transcriber")
    translator = sieve.function.get("sieve/seamless_text2text")
    tts = sieve.function.get("sieve/xtts-v1")
    lipsyncer = sieve.function.get("sieve/video_retalking")

    # transcribe audio
    transcript = list(transcriber.run(source_audio))
    text = transcript_to_text(transcript)

    # Translate text
    translated_text = translator.run(
        text, "eng", seamless_language_map[language])

    # Generate new audio from translated text
    target_audio = tts.run(source_audio, language, translated_text)

    # Combine audio and video with Retalker
    return lipsyncer.run(source_video, target_audio)


if __name__ == "__main__":
    video = sieve.Video(url="https://storage.googleapis.com/sieve-public-data/assets/dub.m4a")
    dubbed_video = video_dubbing(video, "es")
    print(dubbed_video)
```

Now run the code

```jsx
python video_dubbing.py
```

Run the **`video_dubbing.py`** script and the resulting video URL will be printed to the console.

If it ran correctly, great! However, you may encounter an error similar to `command not found: ffmpeg`, which means that ffmpeg is not installed on your machine.

In that case, you can choose to run your code on Sieve’s cloud. Sieve let’s you define and run arbitrary code in the cloud, including this code which calls other Sieve functions. We’ll walk through this functionality more in detail and separate part of our documentation, that can be found here.

To accomplish that, add the following decorator to the function and modify the running code accordingly:

```python
import sieve

def extract_audio(source_video: sieve.Video):
    import subprocess
    audio_path = 'temp.wav'
    subprocess.run(["ffmpeg", "-i", source_video.path, audio_path, "-y"])
    return sieve.Audio(path=audio_path)

def transcript_to_text(transcript):
    transcript = [segment for sublist in transcript for segment in sublist]
    text = " ".join([segment["text"] for segment in transcript])
    return text

seamless_language_map = {
    "en": "eng",
    "es": "spa",
    "fr": "fra",
    "de": "deu",
    "it": "ita",
    "pt": "por",
    "pl": "pol",
    "tr": "tur",
    "ru": "rus",
    "nl": "nld",
    "cs": "ces",
    "ar": "ara",
    "zh-cn": "cmn"
}

@sieve.function(
    name="video-dubber",
    system_packages=["ffmpeg"],
    metadata=metadata,
)
def video_dubbing(source_video: sieve.Video, language: str):
    """
    :param source_video: The video to dub
    :param language: The language to dub the video in
    :return: The dubbed video
    """

    # Extract audio from video
    source_audio = extract_audio(source_video)

    # check if language is supported
    if language not in ["en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru", "nl", "cs", "ar", "zh-cn"]:
        raise Exception("Language not supported")

    # use remote sieve models
    transcriber = sieve.function.get("sieve/speech_transcriber")
    translator = sieve.function.get("sieve/seamless_text2text")
    tts = sieve.function.get("sieve/xtts-v1")
    lipsyncer = sieve.function.get("sieve/video_retalking")

    # transcribe audio
    transcript = list(transcriber.run(source_audio))
    text = transcript_to_text(transcript)

    # Translate text
    translated_text = translator.run(text, "eng", seamless_language_map[language])

    # Generate new audio from translated text
    target_audio = tts.run(source_audio, language, translated_text)

    # Combine audio and video with Retalker
    return lipsyncer.run(source_video, target_audio)

if __name__ == "__main__":
    video = sieve.Video(url="https://storage.googleapis.com/sieve-public-data/assets/dub.m4a")
    dubbed_video = video_dubbing.run(video, "es")
    print(dubbed_video)
```

Running your code on Sieve is as simple as that. Now, if you run the **`video_dubbing.py`** script again, your code will be built and uploaded to Sieve and your function will be executed in the cloud. You will then see the result of your function in the output. 

By following these steps, you created a complex app using many models. Nice!
